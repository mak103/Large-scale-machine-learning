{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Large Scale Machine Learning\n",
    "\n",
    "### Authors: \n",
    "#### Kevin Elgui (kevin.elgui@gmail.com), Pascal Bianchi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolocation challenge\n",
    "\n",
    "The goal of this challenge is to develop a system which tries to perform a geolocation of connected devices within an IoT Network.\n",
    "\n",
    "The challenge will have two phases. Please read the below information carefully.\n",
    "\n",
    "In the first phase of the challenge, you will be provided a training dataset which contains the raw messages and their corresponding coordinates. We will also provide you a validation dataset, which only contains the raw messages. The labes of the validation set will not be provided. In this phase, you will train your system based on this data and the ranking in the challenge webpage will be based on your score obtained on the validation data.\n",
    "\n",
    "The second phase of the challenge will start only a couple of hours before the deadline. Within this period, we will provide you the test set. You will need to run the algorithm that you developed on the first phase on this dataset and submit your predictions to the challenge webpage. Your final ranks will be determined on this dataset. Note that this phase will only last a couple of hours, which means that you will not be able to tune your algorithm on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The properties of the dataset:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set contains raw messages (lines) and their labels (GPS-position). There are ?? messages in this dataset. Each message is of size 1 x 20, where 20 is the number of message descriptors. For each message there is a label, a couple (latitude, longitude). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation set only contains raw messages. There are ?? messages in this dataset, where the format is the same as before. The labels of this dataset will not be provided. Within the first phase, your ranks will be calculated based on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set only contains raw messages as well. There will be ?? messages in this dataset, the format is always the same. The labels of this dataset will not be provided either. This dataset will be provided only in the second phase: only a couple of hours before the challenge deadline. Your final ranks will be calculated based on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The performance criterion¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question of a suitable criterion for evaluation is tough. In practice, in order to suit industrial requirements, we evaluate your prediction throught the 90% quantile of the error vector, which is computed as follows:\n",
    "$$ error = d_V(pred, true),$$ where $d_V$ is the Vincenty distance between to GPS-coordinates. Finally:\n",
    "\n",
    "$$ criterion =  error_{(90\\%)}$$\n",
    "In other words, this criterion corresponds to the max error distance made by your model on $90\\%$ of the test set.\n",
    "The lower the score, the better the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw messages: lien_to_raw_messages_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPS-coordinates: lien_to_label_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw messages: lien_to_raw_messages_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from geopy.distance import vincenty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and investigate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_messages_fname = '../Data/Toy/db_toy.csv'\n",
    "assert(os.path.exists(toy_messages_fname))\n",
    "toy_labels_fname = '../Data/Toy/labels_toy.csv'\n",
    "assert(os.path.exists(toy_labels_fname))\n",
    "\n",
    "toy_messages_df = pd.read_csv(toy_messages_fname, sep=';', low_memory=False)\n",
    "toy_labels_df = pd.read_csv(toy_labels_fname, sep=';', low_memory=False)\n",
    "\n",
    "#toy_messages_fname.head()\n",
    "toy_messages_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_messages_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_messages_fname = '../Data/Train/db_train.csv'\n",
    "assert(os.path.exists(train_messages_fname))\n",
    "train_labels_fname = '../Data/Train/label_train.csv'\n",
    "assert(os.path.exists(train_labels_fname))\n",
    "val_messages_fname  = '../Data/Validation/db_val.csv'\n",
    "assert(os.path.exists(val_messages_fname))\n",
    "val_labels_fname = '../Data/Validation/label_val.csv'\n",
    "\n",
    "train_messages_df = pd.read_csv(train_messages_fname, sep=';', low_memory=False)\n",
    "train_labels_df = pd.read_csv(train_labels_fname, sep=';', low_memory=False)\n",
    "\n",
    "train_messages_fname.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization (requires bokeh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## If you have bokeh installed, you can uncomment the last commands. \n",
    "\n",
    "sys.path.append('../Utils/')\n",
    "\n",
    "#from plot_map import PlotMap\n",
    "#PlotMap(toy_messages_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example prediction\n",
    "\n",
    "In the following, we will train a simple regressor on toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = pd.read_csv('../Data/Toy/db_toy.csv', sep=';')\n",
    "toy_labels = pd.read_csv('../Data/Toy/labels_toy.csv', sep=';', usecols=['latitude', 'longitude'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data['messageid'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data['time_ux'][0]-toy_data['time_ux_client'][0]#重要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = toy_data.groupby('bsid')\n",
    "cl.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features selection\n",
    "\n",
    "In this example, we propose a simple feature matrix $X$ such that :\n",
    "\n",
    "$$\\begin{cases} X[i,j] &= 1 &\\text{ if the $j^{th}$ basestation received the $i^{th}$ message}, \\\\\n",
    " X[i,j] &= 0, &\\text{otherwise}. \\end{cases} $$\n",
    "\n",
    "For the project, you are free to chose other parameters for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please run the cell\n",
    "## --------------------\n",
    "\n",
    "features_of_interest = {\"features_of_interest\": [\"rssi\"],\n",
    "\"target\":[\"latitude\", \"longitude\"]}\n",
    "\n",
    "dict_of_gby = {'rssi': ['bsid'],\n",
    "                'freq': ['bsid'],\n",
    "                'latitude_bs': ['bsid'],\n",
    "                'longitude_bs': ['bsid'],\n",
    "                'latitude': [''],\n",
    "                'longitude': [''],\n",
    "                'speed': [''],\n",
    "                'dtid': [''], \n",
    "                'did': [''],\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from features_builder import _build_features_dict, my_parser\n",
    "(feature_dict, feature_name), (target_dict, target_name)  = _build_features_dict(toy_data, features_of_interest)\n",
    "res = my_parser(toy_data, toy_labels, feature_dict, target_dict, features_of_interest)\n",
    "X = pd.DataFrame(res['X'].toarray(), columns=res['dict_X'])\n",
    "y = pd.DataFrame(res['y'].toarray(), columns=res['dict_y'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)  \n",
    "\n",
    "pred = pd.DataFrame(reg.predict(X_test), columns=res['dict_y'])\n",
    "#print('The criterion on the testing data : %.2f km' % criterion(pred, y_test))\n",
    "\n",
    "#np.savetxt('../Data/Toy/pred_toy.txt', pred.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error(pred.values, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: \n",
    "The criterion is about 20 km. It's huge ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation \n",
    "After having upload your file pred_test.txt on the server, you will be ranked w.r.t the criterion. \n",
    "The score which is assigned to you is computed on half of the test set. This corresponds to the public score. \n",
    "On the other half, you are assigned a private score. \n",
    "\n",
    "Your final score will take into account these two scores. Thus, your final rank may differ slightly from the one you see on the leader board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it's your turn. Good luck !  :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
